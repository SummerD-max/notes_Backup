# FedAT ï¼ˆæœ‰ç‚¹åƒ Hierarchical Fedrated Learningï¼‰

![img](https://img2023.cnblogs.com/blog/2145900/202302/2145900-20230228100858810-1937790399.png)

## Constraint Termï¼ˆä¿è¯æ¨¡å‹ä¸åç§»ï¼‰

![img](https://img2023.cnblogs.com/blog/2145900/202302/2145900-20230228094422453-458358020.png)

é€šè¿‡åœ¨æœ¬åœ°æ›´æ–°ä¸­æ·»åŠ è¿™ä¸ªæ­£åˆ™é¡¹ï¼Œä¿è¯æœ¬åœ°æ¨¡å‹å’Œå…¨å±€æ¨¡å‹ç›¸è¿‘ã€‚

$w_k$ å®¢æˆ·ç«¯kçš„æ¨¡å‹ $w$å…¨å±€æ¨¡å‹ã€‚

## Cross-Tier Weighted Aggregationï¼ˆè®¾ç½®æƒé‡ï¼‰

![img](https://img2023.cnblogs.com/blog/2145900/202302/2145900-20230228095431423-1401202117.png)

FedAT uses a new cross-tier, weighted aggregation heuristic, which dynamically adjusts the relative weights assigned to each tier based on the number of times a tier has updated the global mode. The goal of the weighted aggregation heuristic is to **help the global training converge faster**.

- **a relatively slower tier with a tier number ğ‘š would get assigned a relatively larger weight value**

- **avoid potential bias towards a subset of faster tiers**

## Compression

As studied in [39], many compression methods [3, 40] suffer from slow convergence rates in the Non-i.i.d. cases. Non-i.i.d. often introduces divergence of model weights collected from resource-heterogeneous clients. Due to such divergence, some lossy compression methods such as quantization and dequantization [51] may inevitably lead to huge errors and reduce global performance.

ä½¿ç”¨äº†ä¸€ä¸ªæœ‰æŸçš„å‹ç¼©(lossy compression) - Encoded Polyline Algorithm

- It can be configured to maintain a configurable precision by rounding the value to a specified decimal place. With the appropriate precision, the model could achieve the largest communication saving and minor performance loss.

## ä¸é‡‡å–å¼ºåˆ¶æ›´æ–°ç­–ç•¥