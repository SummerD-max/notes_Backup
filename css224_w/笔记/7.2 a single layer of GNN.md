# A single layer of GNN

## 1. Idea of a GNN Layer:
- Compress a set of vectors into a single vector
- Two step process:
    - 1.Message 
    - 2.aggregation 

## (1) message computation

![img](https://img2022.cnblogs.com/blog/2145900/202211/2145900-20221101151905042-895463789.png)

最简单的方法就是将消息与 W 矩阵做乘法，得到线性变化后的 message $m_{u}^{(l)}$, ($l$) 代表第 $l$ 层的消息

## (2) aggregation

![img](https://img2022.cnblogs.com/blog/2145900/202211/2145900-20221101152358679-1509664614.png)

将转换后的消息 $m_u$ 聚合，聚合函数有
- Aggregation function:
  - Sum()
  - Mean()
  - Max()

## Issue

未考虑 node of interest 上一层的节点嵌入

![img](https://img2022.cnblogs.com/blog/2145900/202211/2145900-20221101152831577-1428512154.png)

## 经典单层 GNN 

### GCN

![img](https://img2022.cnblogs.com/blog/2145900/202211/2145900-20221101153554570-293581431.png)

## GraphSAGE -- GCN 的升级版

![img](https://img2022.cnblogs.com/blog/2145900/202211/2145900-20221101153758412-343895138.png)

（1）考虑到 $\mathbf{AGG}(\cdot)$ 不只有做平均
（2）将 node of interest 本身的上一层嵌入也考虑进去了

### GraphSAGE 相比于 GCN 的新特性

![img](https://img2022.cnblogs.com/blog/2145900/202211/2145900-20221101154247282-670336922.png)

1. AGG 的选择更加多样

AGG 可以是取平均，池化，LSTM

2. 每一层都用了 $\ell_2$ normalization

![img](https://img2022.cnblogs.com/blog/2145900/202211/2145900-20221101154608935-1703171735.png)

- 这样使得每一层的嵌入的欧氏距离都为1
- 在某些情况下能提升模型表现
